name: Newsletter Signup Automation

on:
  schedule:
    # Run daily at 2 PM UTC (avoid peak hours)
    - cron: '0 14 * * *'
  workflow_dispatch:
    inputs:
      limit:
        description: 'Maximum number of domains to process'
        required: false
        default: '500'
        type: string
      industry:
        description: 'Filter by industry (optional)'
        required: false
        type: string
      country:
        description: 'Filter by country (optional)'
        required: false
        type: string
      min_employees:
        description: 'Minimum employee count (optional)'
        required: false
        type: string
      batch_size:
        description: 'Batch size for processing'
        required: false
        default: '100'
        type: string
      max_concurrent:
        description: 'Maximum concurrent sessions'
        required: false
        default: '15'
        type: string
      dry_run:
        description: 'Dry run - fetch domains but dont run automation'
        required: false
        default: false
        type: boolean

env:
  NODE_VERSION: '18'
  PYTHON_VERSION: '3.11'

jobs:
  newsletter-signup:
    runs-on: ubuntu-latest
    timeout-minutes: 300  # 5 hours max
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Set up Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Install Node.js dependencies
      run: |
        npm install playwright csv-parse axios
        npx playwright install chromium
    
    - name: Create BigQuery credentials
      run: |
        echo '${{ secrets.BIGQUERY_CREDENTIALS }}' > bigquery_credentials.json
    
    - name: Set up Browserbase credentials
      run: |
        echo "Configuring Browserbase API access..."
        # The API key and project ID are embedded in the JS file
        # For production, you might want to move these to secrets
    
    - name: Create mailbox accounts file
      run: |
        # Create a basic mailbox accounts CSV if it doesn't exist
        if [ ! -f "mailboxaccounts.csv" ]; then
          echo "Creating default mailbox accounts..."
          cat > mailboxaccounts.csv << 'EOF'
        ÔªøEmail
        ${{ secrets.EMAIL_ACCOUNT_1 || 'test@example.com' }}
        ${{ secrets.EMAIL_ACCOUNT_2 || 'test2@example.com' }}
        ${{ secrets.EMAIL_ACCOUNT_3 || 'test3@example.com' }}
        EOF
        fi
    
    - name: Set automation parameters
      run: |
        # Set parameters from inputs or defaults
        LIMIT="${{ github.event.inputs.limit || '500' }}"
        INDUSTRY="${{ github.event.inputs.industry || '' }}"
        COUNTRY="${{ github.event.inputs.country || '' }}"
        MIN_EMPLOYEES="${{ github.event.inputs.min_employees || '' }}"
        BATCH_SIZE="${{ github.event.inputs.batch_size || '100' }}"
        MAX_CONCURRENT="${{ github.event.inputs.max_concurrent || '15' }}"
        DRY_RUN="${{ github.event.inputs.dry_run || 'false' }}"
        
        echo "AUTOMATION_LIMIT=$LIMIT" >> $GITHUB_ENV
        echo "AUTOMATION_INDUSTRY=$INDUSTRY" >> $GITHUB_ENV
        echo "AUTOMATION_COUNTRY=$COUNTRY" >> $GITHUB_ENV
        echo "AUTOMATION_MIN_EMPLOYEES=$MIN_EMPLOYEES" >> $GITHUB_ENV
        echo "AUTOMATION_BATCH_SIZE=$BATCH_SIZE" >> $GITHUB_ENV
        echo "AUTOMATION_MAX_CONCURRENT=$MAX_CONCURRENT" >> $GITHUB_ENV
        echo "AUTOMATION_DRY_RUN=$DRY_RUN" >> $GITHUB_ENV
        
        echo "üöÄ Newsletter Signup Automation Parameters:"
        echo "   Domain limit: $LIMIT"
        echo "   Industry filter: ${INDUSTRY:-'None'}"
        echo "   Country filter: ${COUNTRY:-'None'}"
        echo "   Min employees: ${MIN_EMPLOYEES:-'None'}"
        echo "   Batch size: $BATCH_SIZE"
        echo "   Max concurrent: $MAX_CONCURRENT"
        echo "   Dry run: $DRY_RUN"
    
    - name: Create logs directory
      run: |
        mkdir -p logs
        echo "üìÅ Created logs directory"
    
    - name: Run newsletter signup automation
      run: |
        echo "üöÄ Starting newsletter signup automation..."
        
        # Build command with parameters
        CMD="python newsletter_signup_bigquery.py"
        CMD="$CMD --limit $AUTOMATION_LIMIT"
        CMD="$CMD --batch-size $AUTOMATION_BATCH_SIZE"
        CMD="$CMD --max-concurrent $AUTOMATION_MAX_CONCURRENT"
        
        if [ ! -z "$AUTOMATION_INDUSTRY" ]; then
          CMD="$CMD --industry '$AUTOMATION_INDUSTRY'"
        fi
        
        if [ ! -z "$AUTOMATION_COUNTRY" ]; then
          CMD="$CMD --country '$AUTOMATION_COUNTRY'"
        fi
        
        if [ ! -z "$AUTOMATION_MIN_EMPLOYEES" ]; then
          CMD="$CMD --min-employees $AUTOMATION_MIN_EMPLOYEES"
        fi
        
        if [ "$AUTOMATION_DRY_RUN" = "true" ]; then
          CMD="$CMD --dry-run"
        fi
        
        echo "üéØ Executing: $CMD"
        eval $CMD
    
    - name: Generate automation report
      if: always()
      run: |
        echo "üìä Generating automation report..."
        python -c "
        import json
        import os
        from datetime import datetime
        from pathlib import Path
        
        print('üìä NEWSLETTER SIGNUP AUTOMATION REPORT')
        print('=' * 50)
        print(f'Run Date: {datetime.now().strftime(\"%Y-%m-%d %H:%M UTC\")}')
        print(f'Parameters: Limit={os.environ.get(\"AUTOMATION_LIMIT\", \"N/A\")}, Batch={os.environ.get(\"AUTOMATION_BATCH_SIZE\", \"N/A\")}, Concurrent={os.environ.get(\"AUTOMATION_MAX_CONCURRENT\", \"N/A\")}')
        print()
        
        # Check if domain metadata exists
        if Path('domain_metadata.json').exists():
            with open('domain_metadata.json', 'r') as f:
                domains = json.load(f)
            print(f'üìã Domains Prepared: {len(domains)}')
            
            # Count by industry
            industries = {}
            countries = {}
            for domain in domains:
                industry = domain.get('metadata', {}).get('industry', 'Unknown')
                country = domain.get('metadata', {}).get('country', 'Unknown')
                industries[industry] = industries.get(industry, 0) + 1
                countries[country] = countries.get(country, 0) + 1
            
            print()
            print('üè≠ Top Industries:')
            for industry, count in sorted(industries.items(), key=lambda x: x[1], reverse=True)[:5]:
                print(f'   {industry}: {count}')
            
            print()
            print('üåç Top Countries:')
            for country, count in sorted(countries.items(), key=lambda x: x[1], reverse=True)[:5]:
                print(f'   {country}: {count}')
        
        # Check for log files
        log_files = ['newsletter_signup.log', 'logs/successful_submissions_production.jsonl', 'logs/failed_submissions_production.jsonl']
        
        for log_file in log_files:
            if Path(log_file).exists():
                size = Path(log_file).stat().st_size
                print(f'üìÑ {log_file}: {size} bytes')
        
        print()
        print('‚úÖ Report generated successfully')
        "
    
    - name: Upload automation logs
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: newsletter-automation-logs-${{ github.run_number }}
        path: |
          logs/
          *.log
          domain_metadata.json
          Storedomains_bigquery.csv
        retention-days: 30
    
    - name: Upload results to BigQuery
      if: success() && github.event.inputs.dry_run != 'true'
      run: |
        echo "üì§ Uploading results to BigQuery..."
        
        # Check if there are results to upload
        if [ -f "logs/successful_submissions_production.jsonl" ] || [ -f "logs/failed_submissions_production.jsonl" ]; then
          python -c "
        import json
        from pathlib import Path
        from newsletter_signup_bigquery import NewsletterSignupOrchestrator
        
        orchestrator = NewsletterSignupOrchestrator()
        
        results = []
        
        # Load successful submissions
        success_file = Path('logs/successful_submissions_production.jsonl')
        if success_file.exists():
            with open(success_file, 'r') as f:
                for line in f:
                    if line.strip():
                        data = json.loads(line)
                        data['success'] = True
                        results.append(data)
        
        # Load failed submissions
        failed_file = Path('logs/failed_submissions_production.jsonl')
        if failed_file.exists():
            with open(failed_file, 'r') as f:
                for line in f:
                    if line.strip():
                        data = json.loads(line)
                        data['success'] = False
                        results.append(data)
        
        if results:
            orchestrator.log_results_to_bigquery(results)
            print(f'‚úÖ Uploaded {len(results)} results to BigQuery')
        else:
            print('‚ÑπÔ∏è No results to upload')
        "
        else
          echo "‚ÑπÔ∏è No result files found to upload"
        fi
    
    - name: Clean up credentials
      if: always()
      run: |
        rm -f bigquery_credentials.json
        echo "üßπ Cleaned up credentials"
    
    - name: Notify on failure
      if: failure()
      run: |
        echo "‚ùå Newsletter signup automation failed!"
        echo "Check the logs and artifacts for details."
        
        # Optional: Add Slack notification
        # curl -X POST -H 'Content-type: application/json' \
        #   --data '{"text":"‚ùå Newsletter signup automation failed in repo ${{ github.repository }}"}' \
        #   ${{ secrets.SLACK_WEBHOOK_URL }}
    
    - name: Success summary
      if: success()
      run: |
        echo "‚úÖ Newsletter signup automation completed successfully!"
        
        # Count results if available
        if [ -f "logs/successful_submissions_production.jsonl" ]; then
          SUCCESS_COUNT=$(wc -l < logs/successful_submissions_production.jsonl)
          echo "üìà Successful signups: $SUCCESS_COUNT"
        fi
        
        if [ -f "logs/failed_submissions_production.jsonl" ]; then
          FAILED_COUNT=$(wc -l < logs/failed_submissions_production.jsonl)
          echo "üìâ Failed signups: $FAILED_COUNT"
        fi 